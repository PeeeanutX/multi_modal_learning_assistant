[PAGE 1]
Logo of the University of Passau
AI-Based Business Information Systems
Explainable AI
Prof. Dr. Ulrich Gnewuch

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] there are two people sitting at a table with laptops and a robot

[IMAGE CAPTION] a close up of a robot with a lot of numbers on it

[IMAGE CAPTION] arafed man and woman looking at a computer screen with a map on it

[PAGE 2]
Logo of the University of Passau
Lecture
AI-Enabled Business Capabilities
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
2
Course Organization
Exercise
Exercise 1: 
Robotic Process 
Automation Case Study
Exercise 2: 
Human-Centered 
Chatbot Design
Exercise 3: 
Explainable AI 
Techniques
Foundations 
Introduction to AI in Business 
& Information Systems
Design & Management of AI-
Based Information Systems
AI-Enabled Engagement
AI-Enabled Insights & Decisions
AI-Enabled Innovation
Industry Talk 
ZF Group
Exercise 4: 
Generative AI & 
Innovation
AI-Enabled Automation 
AI Technologies & Trends
Conversational AI
Explainable AI
Generative AI
AI Ethics & Responsible AI

[IMAGE CAPTION] the university of passau logo

[PAGE 3]
Logo of the University of Passau
RECAP FROM LAST LECTURE:
• Please organize the following concepts 
based on the order in which they 
appear in the information value chain.
• What are key differences between the 
top-down knowledge-driven paradigm 
and the bottom-up data-driven 
paradigm?
• What are typical reasons why decision-
makers ignore AI-enabled insights and 
recommendations?

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a person sitting at a table reading a book

[IMAGE CAPTION] the mentimeer logo with a blue and pink geometric design

[PAGE 4]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
4
Learning Goals
•
Explain the concept of explainable AI (XAI) 
and its historical roots
•
Describe the relationship between XAI 
stakeholders’ explainability needs and the 
design of explanations
•
Distinguish between different XAI approaches 
and name popular techniques 
•
Discuss the challenges and limitations of 
current XAI approaches

[IMAGE CAPTION] the university of passau logo

[PAGE 5]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
5
Why is Explainability Important?
Goodman & Flaxman 2017
Rejected
AI is increasingly used to make 
consequential decisions about us
Companies need to comply with 
European Union regulation 
“right to explanation”

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] arafed robot standing in a room with a blue eye

[IMAGE CAPTION] a close up of a flag with yellow stars in the middle

[PAGE 6]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
6
Why is Explainability Important?
Liao & Varshney 2021; Hind 2019
Decision-makers need to know how 
much they can rely on AI output
Developers want to debug and 
improve their AI models

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] arafed image of a man sitting at a computer with a speech bubble above his head

[IMAGE CAPTION] a close up of a laptop with a magnifying image of a computer

[PAGE 7]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
7
Explainability & Explainable AI (XAI)
•
Explainability is often used interchangeably with other terms, such as interpretability or transparency, 
but there are differences
•
Explainability is not just a (technical) property of a machine learning (ML) model but also considers 
the human side of explanations
Explainable AI (XAI) is the ability of AI-based systems 
to explain their behaviors in understandable terms to 
humans. (based on Du et al. 2020)
Explainability is the ability for humans to understand 
the algorithm’s behavior. (based on Rosenfeld & Richardson 2019)
Miller 2019; Gunning & Aha 2019; Berente et al. 2021

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a close up of a dictionary with a red word definition

[PAGE 8]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
8
Brief History of Explainability
Gregor & Benbasat 1999; Mueller et al. 2019
Time
DARPA’s 
Explainable 
AI Program
1970
Expert Systems
1990
2010
1980
2000
2020
Machine
Learning
Explainable AI is not a new topic. 
The problem of explainability is as old as AI itself.

[IMAGE CAPTION] the university of passau logo

[PAGE 9]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
9
Explanations in Expert Systems
Ye & Johnson 1995; Dhaliwal & Benbasat 1996

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a black square with a white border on it

[IMAGE CAPTION] a close up of a document with a picture of a person

[IMAGE CAPTION] a black square with a white border on it

[IMAGE CAPTION] a white paper with a black title and a black and white image

[PAGE 10]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
10
Explanations in Expert Systems: MYCIN Example
•
MYCIN was one of the best-known 
expert systems (developed in the 1970s)
•
It was designed to provide advice for 
physicians regarding diagnosis and 
therapy for infectious diseases
•
MYCIN offered two types of explanations:
–
Users could ask “HOW” in response to a 
recommendation and receive a trace of 
the rules fired
–
Users could ask “WHY” in response to 
being asked a question by the system, in 
which case MYCIN would provide a trace 
of the currently active goal
Buchanan & Shortliffe 1984

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a black square with a black border and a white border

[IMAGE CAPTION] a black and white image of a text description of a poem

[IMAGE CAPTION] a close up of a black square with a white border

[IMAGE CAPTION] a black and white image of a text description of a type of infection

[PAGE 11]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
11
Today, Explainability is More Difficult (and More Important)
https://www.youtube.com/watch?v=Tsvxx-GGlTg
Neural network 
for recognizing 
handwritten digits
(MNIST dataset)
How can we explain machine learning models?

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] there are many people standing in a circle with a kite

[PAGE 12]
Logo of the University of Passau
Explainability Needs & 
Explanation Design
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
12

[IMAGE CAPTION] the university of passau logo

[PAGE 13]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
13
XAI Stakeholders and Their Explainability Needs
AI Developers
(debug and improve 
AI models)
Decision-Makers
(make informed decisions 
based on an AI application)
Regulatory Bodies
(ensure that the AI is 
safe, and society is not 
negatively impacted)
Business Owners and 
Senior Management
(assess an AI application’s capability, 
regularity compliance, …)
Impacted Groups 
(seek recourse or 
contest the AI)
…
Customers
Patients
Applicants
…
Liao & Varshney 2021; Hind 2019

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a person with a question mark on their head

[PAGE 14]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
14
XAI Stakeholders and Their Explainability Needs
AI Developers
(debug and improve 
AI models)
Decision-Makers
(make informed decisions 
based on an AI application)
Regulatory Bodies
(ensure that the AI is 
safe, and society is not 
negatively impacted)
Business Owners and 
Senior Management
(assess an AI application’s capability, 
regularity compliance, …)
Impacted Groups 
(seek recourse or 
contest the AI)
…
Customers
Patients
Applicants
…
Liao & Varshney 2021; Hind 2019
→There can be no “one-fits-all” solution to XAI!

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a person with a question mark on their head

[PAGE 15]
Logo of the University of Passau
Stakeholders & Explainability Needs in AI-based 
Loan Application Decision-Making
Please imagine the following scenario:
A bank employs an AI system (“RoboLoan”) to assist bank 
consultants in evaluating loan applications submitted by 
private consumers. The system analyzes applicants’ data 
and provides recommendations on whether a loan should be 
approved or rejected.
1.
Who are the relevant XAI stakeholders in this 
scenario?
2.
What are their specific explainability needs?
→Discuss these questions with a partner for ~5  
minutes and be ready to share your answers

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] three people with speech bubbles on their heads

[PAGE 16]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
16
Explainability Needs Expressed as Questions
Liao et al. 2020; Liao & Varshney 2021
Task objectives
Main stakeholders who 
engage in this task
Example questions they may ask the AI
Debug and improve AI 
models
Model Developers
• Is the AI’s performance good enough?
• How does the AI make predictions? How might it go wrong?
• Why does the AI make such a mistake?
To evaluate AI’s 
capability and form 
appropriate trust
All stakeholders may engage in 
this task at some point
• Is the AI’s performance good enough? What are the risks and limitations?
• What kinds of output can the AI give?
• How does the AI work? Is it reasonable?
Make informed 
decisions or take 
better actions
Decision-Makers, Impacted 
Groups
• Why is this instance predicted to be X?
• Why is this instance not predicted to be Y?
• How to change this instance to be predicted Y?
To adapt usage or 
control
Decision-Makers, Business 
Owners / Senior Management
• How does the AI make predictions? What can I supply or change for it to 
work well?
• What if I make this change?
Ensure ethical or legal
compliance
Regulatory Bodies
• How does the AI make predictions? Are there any legal/ethical concerns, 
such as discrimination, privacy, or security concerns?
• Why are the two instances/groups not treated the same by the AI?
…
…
…

[IMAGE CAPTION] the university of passau logo

[PAGE 17]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
17
Question-Driven XAI
•
How: asking about the general logic or process the AI follows (to have a global view)
•
Why: asking about the reason behind a specific prediction
•
Why Not: asking why the prediction is different from an expected or desired outcome
•
How to change to be that: asking about ways to change the instance to get a different prediction
•
How to remain to be this: asking what change is allowed for the instance to still get the same 
prediction
•
What if: asking how the prediction changes if the input changes
•
Performance: asking about the performance of the AI
•
Data: asking about the training data
•
Output: asking what can be expected or done with the AI’s output
Liao et al. 2020

[IMAGE CAPTION] the university of passau logo

[PAGE 18]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
18
Mapping Between Questions and Explanations
Questions
Possible Ways to Explain
How
(global model-wide)
• Describe the general model logic (e.g., as feature impact, rules, or decision-trees)
• If a user is only interested in a high-level view, describe what are the top features or rules considered 
Why
• Describe what key features of the instance determine the model’s prediction of it
• Describe rules that the instance fits to guarantee the prediction
• Show similar examples with the same predicted outcome to justify the model’s prediction
Why not
• Describe what changes are required for the instance to get the alternative prediction and/or what features of the instance 
guarantee the current prediction
• Show prototypical examples that had the alternative outcome
How to be that
(a different prediction)
• Highlight features that if changed (increased, decreased, absent, or present) could alter the prediction
• Show examples with minimum differences but had a different outcome than the prediction
How to still be this
(the current prediction)
• Describe features/feature ranges or rules that could guarantee the same prediction
• Show examples that are different from the particular instance but still had the same outcome
What if
• Show how the prediction changes corresponding to the inquired change
Performance
• Provide performance metrics of the model
• Show uncertainty information for each prediction
Data
• Document comprehensive information about the training data, including the source, provenance, type, size, coverage of 
population, potential biases, etc.
Output
• Describe the scope of output or system functions
Liao & Varshney 2021

[IMAGE CAPTION] the university of passau logo

[PAGE 19]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
19
Explanation Characteristics & Design
Explanations
Content
Presentation 
Format
Provision 
Mechanism
Timing
…
Interactivity

[IMAGE CAPTION] the university of passau logo

[PAGE 20]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
20
Explanation Presentation Format
Explanations are provided in 
natural language:
•
Rules of an expert system
•
Plain natural language
•
…
Explanations are provided in 
non-text format: 
•
Graphs
•
Histograms
•
Images
•
…
Presentation 
Format
Text-based
Multimedia
Gregor & Benbasat 1999

[IMAGE CAPTION] the university of passau logo

[PAGE 21]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
21
Explanation Presentation Format: Examples
Saliency Maps
“If the applicant’s income had 
been $10,000 higher, the loan 
would have been approved”
Counterfactual Explanations

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a close up of a picture of a group of animals in a field

[PAGE 22]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
22
Explanation Provision Mechanism
Explanations are provided at 
the requests of users.
Explanations are provided to users 
by the AI when it considers such
explanations are needed.
Provision 
Mechanism
User-Invoked
Automatic
Intelligent
Explanations are provided 
automatically without users’ 
requests.
Gregor & Benbasat 1999

[IMAGE CAPTION] the university of passau logo

[PAGE 23]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
23
User-Invoked Explanation Provision: Example
Facebook’s “Why am I seeing this ad?”
ChatGPT’s 
Code Interpreter

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a screenshot of a text message from a customer asking to sell a product

[IMAGE CAPTION] a close up of a computer screen with a number of items

[IMAGE CAPTION] two iphones with a facebook page on the screen and a facebook page on the screen

[IMAGE CAPTION] the logo for the center for the arts and design

[PAGE 24]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
24
Explanation Timing
Explanations are provided
before a certain behavior is 
performed by the AI.
Explanations are provided
after a certain behavior is 
completed by the AI.
Timing
Feedforward 
Explanation
Feedback     
Explanation
Dhaliwal & Benbasat 1996

[IMAGE CAPTION] the university of passau logo

[PAGE 25]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
25
Interactivity
Static Explanation
Interactive Explanation Interface
Shneiderman 2020

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a diagram of a bar chart showing the amount of a mortgage account

[IMAGE CAPTION] a screenshot of a calculator with a sign that says enter amounts to request mortgage

[IMAGE CAPTION] a screenshot of a cell phone showing a mortgage request

[PAGE 26]
Logo of the University of Passau
Explainable AI 
Approaches
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
27

[IMAGE CAPTION] the university of passau logo

[PAGE 27]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
28
Overview of Explainable AI Approaches
XAI Approaches
Intrinsic 
Explainability
…
…
…
Guidotti et al. 2019
Post-Hoc 
Explainability
Choosing a directly 
explainable AI model 
(“white-box model”)
Choosing a not self-explanatory​ AI model (“black-box model”) and 
then using a post-hoc technique to generate explanations

[IMAGE CAPTION] the university of passau logo

[PAGE 28]
Logo of the University of Passau
•
White-box models
incorporate explainability 
directly into their structures
•
Examples: decision tree, 
linear/logistic regression, 
rule-based models
•
Sometimes not possible and 
can also get quite complex 
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
29
Intrinsic Explainability: White-Box Models
Du et al. 2019
income > 50.000€
credit score > 75
Reject
Yes
Yes
No
No
Reject
Approve
Decision path 
(for a single decision)
Splitting Criteria
(for the overall model)
Decision Tree

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a diagram of a tree with four different trees

[PAGE 29]
Logo of the University of Passau
Why not always choose a white-box model?
•
Black-box models often outperform white-
box models due to their ability to capture 
high non-linearity and interactions between 
features
•
The choice between the two is discussed 
under the term “performance–explainability 
tradeoff”
•
This tradeoff is not always true: In many 
contexts, especially with well-structured 
datasets and meaningful features, white-box 
models can reach comparable performance
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
30
Performance – Explainability Tradeoff
Herm et al. 2023; Liao & Varshney 2021

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a diagram of a line graph with the different types of networked devices

[PAGE 30]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
31
Black Box Models
•
Black-box models are complex and not self-
explanatory​
•
Examples: neural networks, ensemble models
•
Post-hoc explainability techniques can be 
used to generate explanations for their output 
(e.g., predictions)
–
Can be applied to any model (model-agnostic)
–
But usually an approximation and not 
always faithful!
Reject
?
Rudin 2019

[IMAGE CAPTION] the university of passau logo

[PAGE 31]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
32
XAI Approaches: Post-Hoc Explainability
32
XAI Approaches
Intrinsic 
Explainability
Post-Hoc 
Explainability
Explaining a Single 
Output (Local)
Explaining the 
Model (Global)
Inspecting 
Counterfactual
Guidotti et al. 2019; Du et al. 2019
Explaining the overall 
logic of the model
(e.g., feature importance)
Explaining a particular 
model output (e.g., local 
feature contribution)
Explaining how the model 
would behave with an 
alternative input (e.g., 
contrastive feature)

[IMAGE CAPTION] the university of passau logo

[PAGE 32]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
33
Global Explanations: Examples
Partial Dependence Plot
Permutation Feature Importance
https://christophm.github.io/interpretable-ml-book/pdp.html

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a diagram of the temperature and humidity of a hot house

[IMAGE CAPTION] a graph showing the number of people in a business setting

[PAGE 33]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
34
Local Explanations: Examples
SHAP
LIME
https://christophm.github.io/interpretable-ml-book/local-methods.html
(SHAP values can be used for both 
local and global explanations)

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] three pictures of a fish swimming in the ocean with a green fish

[IMAGE CAPTION] a close up of a graph with a line of data on it

[PAGE 34]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
35
LIME (Local Interpretable Model-Agnostic Explanations)
Ribeiro et al. 2016
1. Select an instance for which you need an explanation of its black-box model prediction
2. Create a dataset of similar instances (“perturbing”)
3. Get the black-box model predictions for all these instances
4. Train a white-box model (“surrogate model”) on the new dataset consisting of instances and 
corresponding black-box model predictions 
5. Use the white-box model to generate explanations for the black-box model’s prediction
Data
Black Box 
Model
Predictions
White Box 
Model
Explanations
(surrogate model)
2
3
4
5

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a black and white image of a magnifying book with a magnifying glass

[PAGE 35]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Example: LIME
Applicant 
Information
Amount (EUR]
2.835€
Duration (months)
24
Purpose
furniture/equipment
Checking Account
no checking account 
at this bank
Loan History
previous loans paid 
back duly
Employment
Longer than 7 years
Checking Account
(no checking account)
Loan History (previous 
loans paid back duly)
Employment
(longer than 7 years)
Amount (>= 2.500€)
Prediction: Rejected
LIME Output
(four most important 
features shown)
Chair of Information Systems (Explainable AI-based Business Information Systems)
36
36
LIME Python package: https://github.com/marcotcr/lime
Dataset: https://doi.org/10.24432/C5NC77

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a close up of a bar chart with a number of different numbers

[IMAGE CAPTION] a close up of a bar chart with a blue rectangle and a red rectangle

[IMAGE CAPTION] a close up of a black background with a blue and orange text

[PAGE 36]
Logo of the University of Passau
LIME Results
Examine the explanation generated by LIME for 
the loan application data on the previous slide.
1.
What do you think is the most important 
feature in this prediction (0.18)?
2.
Which features do you think support the 
approval of the loan application (orange) and 
which ones contribute to the rejection (blue)?
→Discuss these questions with a partner for 2-3 
minutes and be ready to share your answers

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] three people with speech bubbles on their heads

[PAGE 37]
Logo of the University of Passau
Example: LIME
Applicant 
Information
Amount (EUR]
2.835€
Duration (months)
24
Purpose
furniture/equipment
Checking Account
no checking account 
at this bank
Loan History
previous loans paid 
back duly
Employment
Longer than 7 years
Checking Account
(no checking account)
Loan History (previous 
loans paid back duly)
Employment
(longer than 7 years)
Amount (>= 2.500€)
Prediction: Rejected
LIME Output
(four most important 
features shown)
Prof. Dr. Ulrich Gnewuch
38
Chair of Information Systems (Explainable AI-based Business Information Systems)
38
LIME Python package: https://github.com/marcotcr/lime
Dataset: https://doi.org/10.24432/C5NC77

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a close up of a bar chart with a number of different numbers

[IMAGE CAPTION] a close up of a bar chart with a blue rectangle and a red rectangle

[IMAGE CAPTION] a close up of a black background with a blue and orange text

[PAGE 38]
Logo of the University of Passau
•
Counterfactual explanations provide an 
understanding of model outputs by posing 
hypothetical “what if” scenarios
•
Counterfactuals show how the prediction 
would change if certain features were 
different
Example:
“If the applicant’s income had been $10,000 
higher, the loan application would have been 
approved.”
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
39
Counterfactual Explanations: Example
Fernández-Loría et al. 2022

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a diagram of a road with a line of different types of items

[PAGE 39]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
40
Mapping Between Questions, Explanations, and XAI Techniques
Questions
Possible Ways to Explain
How
(global model-wide)
• Describe the general model logic (e.g., as feature impact, rules, or decision-trees)
• If a user is only interested in a high-level view, describe what are the top features or rules considered 
Why
• Describe what key features of the instance determine the model’s prediction of it
• Describe rules that the instance fits to guarantee the prediction
• Show similar examples with the same predicted outcome to justify the model’s prediction
Why not
• Describe what changes are required for the instance to get the alternative prediction and/or what features 
of the instance guarantee the current prediction
• Show prototypical examples that had the alternative outcome
How to be that
(a different prediction)
• Highlight features that if changed (increased, decreased, absent, or present) could alter the prediction
• Show examples with minimum differences but had a different outcome than the prediction
How to still be this
(the current prediction)
• Describe features/feature ranges or rules that could guarantee the same prediction
• Show examples that are different from the particular instance but still had the same outcome
What if
• Show how the prediction changes corresponding to the inquired change
Liao & Varshney 2021
Note: Permutation Feature Importance (PFI), Partial Dependence Plot (PDP), SHapley Additive exPlanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), Contrastive 
Explanations Method (CEM) 
Example XAI 
Technique
PFI, PDP, SHAP, …
SHAP, LIME, …
Counterfactuals, 
CEM, …
Counterfactuals, 
CEM, …
CEM, …
PDP, …

[IMAGE CAPTION] the university of passau logo

[PAGE 40]
Logo of the University of Passau
•
The development of new XAI 
techniques is a rapidly evolving 
and highly active research area
•
A comprehensive overview of 
existing XAI techniques can be 
found here: 
https://kdd-lab.github.io/XAISurvey/
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
41
More XAI Techniques …
Bodria et al. 2023

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a black square with a white border on it

[IMAGE CAPTION] a close up of a web page with a bunch of different items

[PAGE 41]
Logo of the University of Passau
Challenges and 
Limitations of XAI
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
42

[IMAGE CAPTION] the university of passau logo

[PAGE 42]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
43
Miscalibrated Trust & Inappropriate Reliance
Explanations can lead to a false sense 
of confidence and unwarranted trust
vs.
Explanations can make people 
lose trust in AI and under-rely on it
Bauer et al. 2023; Zhang et al. 2020; Ostinelli et al. 2024

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a close up of a sign with a question mark on it

[IMAGE CAPTION] a close up of a thumb up with a black background

[PAGE 43]
Logo of the University of Passau
•
Overly complex and detailed 
explanations can cause 
information overload
•
This creates frustration and 
confusion
•
People might misinterpret or not 
fully understand the 
explanations
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
44
Information Overload & Lack of Expertise
de Bruijn 2022; Poursabzi-Sangdeh et al. 2021

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a black square with a white border and a black background

[IMAGE CAPTION] there are four different types of logos that are used to describe the company

[PAGE 44]
Logo of the University of Passau
•
Explanations may enable 
people to “game” the AI 
system
•
If people have access to 
information about how a 
decision or recommendation 
has been made, they might 
alter their behavior to gain a 
more favorable outcome
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
45
“Gaming the System”
Khosravi et al. 2020
“If the applicant’s income had been 
$10,000 higher, the loan application 
would have been approved.”

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a diagram of a bar chart showing the amount of a mortgage account

[PAGE 45]
Logo of the University of Passau
•
Providing detailed explanations might risk 
exposing proprietary information about the 
model’s architecture or training data
–
This could create tensions between 
transparency and competitive advantage
•
Explanations can also introduce privacy risks by 
inadvertently revealing sensitive information 
embedded in the model’s training data
–
This could create tensions between 
transparency and data protection
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
46
Data Privacy and Intellectual Property Concerns
Goethals et al. 2023

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a person ' s hand reaching out to touch a laptop with a lock on it

[PAGE 46]
Logo of the University of Passau
•
XAI is a long-standing, sociotechnical challenge 
involving a technical side and a human side
•
Different people have different explainability needs 
(e.g., developer vs. decision-maker vs. customer)
•
Explanations can differ in a variety of ways (e.g., 
content, presentation format, timing)
•
Two main approaches to XAI exist: intrinsic
explainability and post-hoc explainability
–
There are many different post-hoc XAI techniques 
(e.g., LIME)
•
Despite its benefits, XAI has several downsides 
(e.g., information overload, data privacy concerns) 
and risks (e.g., miscalibrated trust, overreliance)
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
47
Key Takeaways From This Lecture

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a close up of a check mark on a check mark on a white background

[PAGE 47]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
48
Thank you for 
your attention!
Any questions?

[IMAGE CAPTION] the university of passau logo

[IMAGE CAPTION] a cartoon robot with a yellow tail waving

[PAGE 48]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
49
References (1)
Bauer, K., von Zahn, M., & Hinz, O. (2023). Expl (AI) ned: The impact of explainable artificial intelligence on users’ information processing. Information systems research, 34(4), 
1582-1602.
Berente, N., Gu, B., Recker, J., & Santhanam, R. (2021). Managing artificial intelligence. MIS quarterly, 45(3).
Bodria, F., Giannotti, F., Guidotti, R., Naretto, F., Pedreschi, D., & Rinzivillo, S. (2023). Benchmarking and survey of explanation methods for black box models. Data Mining and 
Knowledge Discovery, 37(5), 1719-1778.
Buchanan, B., & Shortliffe, E. (1984). Rule Based Expert Systems: The MYCIN Experiments of the Stanford Heuristic Programming Project, Addison Wesley Publishing 
Company, Reading, MA.
de Bruijn, H., Warnier, M., & Janssen, M. (2022). The perils and pitfalls of explainable AI: Strategies for explaining algorithmic decision-making. Government Information 
Quarterly, 39(2), 101666.
Dhaliwal, J. S., & Benbasat, I. (1996). The use and effects of knowledge-based system explanations: theoretical foundations and a framework for empirical evaluation. 
Information systems research, 7(3), 342-362.
Du, M., Liu, N., & Hu, X. (2019). Techniques for interpretable machine learning. Communications of the ACM, 63(1), 68–77.
Fernández-Loría, C., Provost, F., & Han, X. (2022). Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach. MIS Quarterly, 46(3), 1635-1660.
Goethals, S., Sörensen, K., & Martens, D. (2023). The privacy issue of counterfactual explanations: explanation linkage attacks. ACM Transactions on Intelligent Systems and 
Technology, 14(5), 1-24.
Goodman, B., & Flaxman, S. (2017). European Union Regulations on Algorithmic Decision-Making and a “Right to Explanation”. AI Magazine, 38(3), 50-57. 
Gregor, S., & Benbasat, I. (1999). Explanations from Intelligent Systems: Theoretical Foundations and Implications for Practice. MIS Quarterly, 23(4), 497. 
Guidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., & Pedreschi, D. (2018). A survey of methods for explaining black box models. ACM computing surveys (CSUR), 
51(5), 1-42.
Gunning, D., & Aha, D. (2019). DARPA’s explainable artificial intelligence (XAI) program. AI magazine, 40(2), 44-58. https://www.darpa.mil/program/explainable-artificial-
intelligence
Herm, L. V., Heinrich, K., Wanner, J., & Janiesch, C. (2023). Stop ordering machine learning algorithms by their explainability! A user-centered investigation of performance and 
explainability. International Journal of Information Management, 69, 102538.

[IMAGE CAPTION] the university of passau logo

[PAGE 49]
Logo of the University of Passau
Prof. Dr. Ulrich Gnewuch
Chair of Explainable AI-based Business Information Systems
50
References (2)
Hind, M. (2019). Explaining explainable AI. XRDS: Crossroads, The ACM Magazine for Students, 25(3), 16-19.
Khosravi, H., Shum, S. B., Chen, G., Conati, C., Tsai, Y. S., Kay, J., ... & Gašević, D. (2022). Explainable artificial intelligence in education. Computers and Education: Artificial 
Intelligence, 3, 100074.
Liao, Q. V., & Varshney, K. R. (2021). Human-centered explainable ai (xai): From algorithms to user experiences. arXiv preprint arXiv:2110.10790.
Liao, Q. V., Gruen, D., & Miller, S. (2020). Questioning the AI: informing design practices for explainable AI user experiences. In Proceedings of the 2020 CHI conference on 
human factors in computing systems (pp. 1-15).
Miller, T. (2019). Explanation in artificial intelligence: Insights from the social sciences. Artificial intelligence, 267, 1-38.
Mueller, S. T., Hoffman, R. R., Clancey, W., Emrey, A., & Klein, G. (2019). Explanation in human-AI systems: A literature meta-review, synopsis of key ideas and publications, 
and bibliography for explainable AI. https://apps.dtic.mil/sti/citations/trecms/AD1073994
Ostinelli, M., Bonezzi, A., & Lisjak, M. (2024). Unintended effects of algorithmic transparency: The mere prospect of an explanation can foster the illusion of understanding how 
an algorithm works. Journal of Consumer Psychology.
Poursabzi-Sangdeh, F., Goldstein, D. G., Hofman, J. M., Wortman Vaughan, J. W., & Wallach, H. (2021). Manipulating and measuring model interpretability. In Proceedings of 
the 2021 CHI conference on human factors in computing systems (pp. 1-52).
Ribeiro, M. T., Singh, S., & Guestrin, C. (2016, August). " Why should i trust you?" Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD 
international conference on knowledge discovery and data mining (pp. 1135-1144). 
Rosenfeld, A., & Richardson, A. (2019). Explainability in human–agent systems. Autonomous Agents and Multi-Agent Systems, 33, 673-705.
Rudin, C. (2019). Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature machine intelligence, 1(5), 206-
215.
Shneiderman, B. (2020). Bridging the gap between ethics and practice: guidelines for reliable, safe, and trustworthy human-centered AI systems. ACM Transactions on 
Interactive Intelligent Systems (TiiS), 10(4), 1-31.
Ye, L. R., & Johnson, P. E. (1995). The impact of explanation facilities on user acceptance of expert systems advice. MIS Quarterly, 157-172.
Zhang, Y., Liao, Q. V., & Bellamy, R. K. (2020). Effect of confidence and explanation on accuracy and trust calibration in AI-assisted decision making. In Proceedings of the 
2020 conference on fairness, accountability, and transparency (pp. 295-305).

[IMAGE CAPTION] the university of passau logo
