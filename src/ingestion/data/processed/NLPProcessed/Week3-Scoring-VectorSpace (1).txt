[PAGE 1]
UniversitÃ¤t Konstanz
Bereich fÃ¼r Partnerlogo
Auf eine Ausgewogenheit zwischen den gleichberechtigten 
Logos ist unbedingt zu achten. Gegebenenfalls muss die 
GrÃ¶ÃŸe des Partnerlogos angepasst werden.
Die GrÃ¶ÃŸe des Logos der UniversitÃ¤t Konstanz darf nicht 
verÃ¤ndert werden.
Annette Hautli-Janisz, Prof. Dr. 
14 November 2024
Information Retrieval &
Natural Language Processing
Week 3: Scoring & vector space models

[PAGE 2]
UniversitÃ¤t Konstanz
Information Retrieval (IR): Today
2
(IIR): Manning, Raghavan and SchÃ¼tze, Introduction to IR, 2008
Chapter 6: Scoring, term weighting and the vector space model

[PAGE 3]
UniversitÃ¤t Konstanz
The classic search model
3
User 
task
Info 
need
User 
query
Search 
engine
Results
Collection
Query refinement
Prepare 
obazda for 
dinner
Find a 
recipe for 
obazda
how to 
make 
obazda
Search
Misconception? Misformulation?
Today

[PAGE 4]
UniversitÃ¤t Konstanz
From Boolean to ranked retrieval
4
So far: Boolean queries â†’Documents either match or donâ€™t. 
Good for
â€¢
expert users (precise understanding of the need and collection)
â€¢
applications (easy consumption of 1000s of results)
Not good for
â€¢
most users (unwilling/incapable to write Boolean queries)
â€¢
web search
Search engine: rank-order the documents matching a query.

[PAGE 5]
UniversitÃ¤t Konstanz
Ranked retrieval
5
The system returns an ordering of the top documents in the collection. 
Typically, free text queries: more than one word in natural language (no 
operators etc.)
â†’If search algorithm works, large sets are not a problem anymore (or at 
least, should not be): 
â€¢
rank result according to usefulness to the user
â€¢
only top k results are shown

[PAGE 6]
UniversitÃ¤t Konstanz
Ranked retrieval
So far: Document = sequence of terms
But: Most documents have additional structure. 
One option: Parametric and zone indexes â€“ they make use of this 
additional structure.
6

[PAGE 7]
UniversitÃ¤t Konstanz
The parametric index
7
A parametric index is an index that allows retrieval of documents based on 
the values of parameters. It serves as an extension of the collectionâ€™s word 
index.

[PAGE 8]
UniversitÃ¤t Konstanz
The parametric index
Make use of metadata associated with each document. 
Fields, e.g. date of creation, with a (finite) set of possible values, e.g. the 
set of all dates of authorship. 
Each field has one parametric index. 
Date of publication index:  
1601 
â†’1   â†’2
1602 
â†’3   â†’4
1932 
â†’5   â†’6
Allows us to to select only the documents matching a data specified in the 
query. 
8

[PAGE 9]
UniversitÃ¤t Konstanz
Zone indexes
Zones are similar to fields, except the contents of a zone can be arbitrary 
free text. They are encoded as extensions of dictionary entries. 
9

[PAGE 10]
UniversitÃ¤t Konstanz
Ranked retrieval without zones
10
The system returns an ordering over the top documents in the collection for 
a query. 
Queries without any operators, formulated in natural language. 
Scoring is the basis of ranked retrieval:
â€¢
assign a score to a query/document pair, based on the weight of the 
query in the document. 
â€¢
aka: the higher the score, the more relevant the document is to the 
query.

[PAGE 11]
UniversitÃ¤t Konstanz
A first take at rankingâ€¦
11
Using the Jaccard coefficient
Measuring the similarity of two sets A (the query) and B (the document) with 
a number between 0 and 1. 
J(A, B) = | A â‹‚B | / | A â‹ƒB| 
J(A, A) = 1
J(A, B) = 0 if A â‹‚B = 0
A and B do not have to have the same size.

[PAGE 12]
UniversitÃ¤t Konstanz
A first take at rankingâ€¦
12
Using the Jaccard coefficient
What is the query-document Jaccard score for each of the two documents 
below?
Query = â€˜ides of marchâ€™
doc1 = â€˜caesar died in marchâ€™
doc2 = â€˜the long marchâ€™
What are the issues?

[PAGE 13]
UniversitÃ¤t Konstanz
Recall: Term-document incidence matrix
13
Possibility 1: Each document is represented as a binary vector [0, 1] â†’
binary matrix.
The Hitch-
hikerâ€™s 
Guide to 
the Galaxy
The 
Restau-
rant at 
the End 
of the 
Universe
Life, the 
Universe 
and 
Every-
thing
So Long, 
and 
Thanks 
for all the 
Fish
Mostly 
Harmless
And 
Another 
Thing...
Arthur
1
1
0
0
0
1
Ford
1
1
0
1
0
0
Zaphod
1
1
0
1
1
1
Trillian
0
1
0
0
0
0
Cleopatra
1
0
0
0
0
0
Marvin
1
0
1
1
1
1
Random
1
0
1
1
1
0

[PAGE 14]
UniversitÃ¤t Konstanz
Recall: Term-document incidence matrix
14
Possibility 2: Number of term occurrences in a document (count vectors are 
columns in the table) â†’count matrix.
The Hitch-
hikerâ€™s 
Guide to 
the Galaxy
The 
Restau-
rant at 
the End 
of the 
Universe
Life, the 
Universe 
and 
Every-
thing
So Long, 
and 
Thanks 
for all the 
Fish
Mostly 
Harmless
And 
Another 
Thing...
Arthur
157
73
0
0
0
5
Ford
4
157
0
1
0
0
Zaphod
232
227
0
2
1
1
Trillian
0
10
0
0
0
0
Cleopatra
57
0
0
0
0
0
Marvin
2
0
3
5
5
1
Random
2
0
1
1
1
0

[PAGE 15]
UniversitÃ¤t Konstanz
Term frequency
Term frequency tft,d of term t in document d is the number of times that t 
occurs in d. 
But: absolute frequencies are not what we want. 
â€¢
Not all terms are equally important. 
â€¢
A document with 10 occurrences is more relevant than a document 
with one occurrence. 
â€¢
But not 10 times more relevant. 
Relevance does not increase proportionally with term frequency. 
â†’Reduce the effect of term frequency in determining relevant docs. 
15

[PAGE 16]
UniversitÃ¤t Konstanz
Term frequency
Choose log-frequency weighting of term t in document d.
1 + log10 tft,d
if tft,d > 0
wt,d =
0
otherwise
(tf = 0 â†’w = 0; tf = 1 â†’w = 1; tf = 2 â†’w = 1,3; tf = 1000 â†’w = 4)
16

[PAGE 17]
UniversitÃ¤t Konstanz
Term frequency
Score for a document-query pair: sum over terms t in both query and 
document. 
score(q,d)
Example:  
q = â€˜ides of marchâ€™, d1 = â€˜caesar died in marchâ€™, d2 = â€˜the long marchâ€™
What is score(q,d1) and score(q,d2)?
17
ïƒ¥
ïƒ‡
ïƒ
+
=
d
q
t
d
t )
 tf
log
 
 
(1
,

[PAGE 18]
UniversitÃ¤t Konstanz
Term frequency
18
Occurrence of each term (`term frequencyâ€™) is central.
E.g., John is quicker than Mary has the same vector as Mary is quicker 
than John. 
But the assumption is: two documents with similar bag of words 
representations are similar in content.

[PAGE 19]
UniversitÃ¤t Konstanz
Collection frequency versus document frequency
We need to scale down the weight of tf in determining relevant documents. 
In other words, we need a weight for each term in the document which 
reflects how much this term helps in distinguishing a document from all 
other documents in the collection.
Two possibilities: 
â€¢
Collection frequency (cf): the total number of occurrences of term t in the 
collection. 
â€¢
Document frequency (df): the number of documents in the collection that 
contain a term t. 
19

[PAGE 20]
UniversitÃ¤t Konstanz
Collection frequency versus document frequency
Example:
Should we prefer collection frequency (cf) or document frequency (df)? 
Why?
20
word
cf
df
try
10422
8760
insurance
10440
3997

[PAGE 21]
UniversitÃ¤t Konstanz
Inverse document frequency (idf)
21
Document frequency dft is an inverse measure of the informativeness of t 
â†’the higher the document frequency (df), the lower the informativeness of 
term t. 
idft = log10 (N
dft)
N is the number of documents in the collection.
Log (N
dft) reduces the effect of the idf weight on the overall term weight.

[PAGE 22]
UniversitÃ¤t Konstanz
Inverse document frequency (idf)
22
Example: N = 1 Million, idft = log10 (N
dft)
term
dft
idft
Calpurnia
1
6
animal
100
4
Sunday
1.000
3
fly
10.000
2
under
100.000
1
the
1.000.000
0

[PAGE 23]
UniversitÃ¤t Konstanz
tf-idf
23
TF-IDF (term frequency - inverse document frequency) 
tf-idft,d = tft,d x idft
Best-know weighting scheme in information retrieval
â€¢
the â€œ-â€ in tf-idf is a hyphen, not a minus sign!
â€¢
alternative names: tf.idf, tf x idf
â€¢
increases with the number of occurrences within a document
â€¢
increases with the rarity of the term in the collection

[PAGE 24]
UniversitÃ¤t Konstanz
tf-idf
24
In other wordsâ€¦
â€¢
term frequency: how many times a term occurs in a document. 
(Assumption: a document containing many times a given word, is likely 
to be about that word) 
â€¢
inverse document frequency: how many documents in the collection 
contain a term. (Assumption: if a term occurs in many documents, it is 
not very discriminative.) 
A survey conducted by Breitinger et al. in 2015 shows that 83% of text-
based recommender systems in digital libraries use tfâ€“idf.

[PAGE 25]
UniversitÃ¤t Konstanz
Score for a document given a query
25
score(q,d) = Ïƒğ‘¡âˆˆğ‘tfâˆ’idft,d
The sum of tf-idf weights for all terms in the query q that are matching in  
document d. 
Different weighting schemes for tf and idf available. We use here: 
wt,d = (1 + log10 tft,d) x log10 (N
dft)

[PAGE 26]
UniversitÃ¤t Konstanz
Term-document weight matrix
26
Possibility 3: Each document is represented by a real-valued vector of tf-idf
weights â†’weight matrix
The Hitch-
hikerâ€™s 
Guide to 
the Galaxy
The 
Restau-
rant at 
the End 
of the 
Universe
Life, the 
Universe 
and 
Every-
thing
So Long, 
and 
Thanks 
for all the 
Fish
Mostly 
Harmless
And 
Another 
Thing...
Arthur
5.25
3.18
0
0
0
0.35
Ford
1.21
6.1
0
1
0
0
Zaphod
8.59
2.54
0
1.51
0.25
0.35
Trillian
0
1.54
0
0
0
0
Cleopatra
2.85
0
0
0
0
0
Marvin
1.51
0
1.9
0.12
5.25
0.88
Random
1.37
0
0.11
4.15
0.25
0

[PAGE 27]
UniversitÃ¤t Konstanz
This is a vector (ğ‘‰(z)) defined over the coordinate axes x and y. 
In IR, vectors are used to represent terms and documents. 
Each document is a vector defined over co-ordinate axes; the co-ordinate 
axes are the terms contained in the document. 
High-dimensional: tens of millions of dimensions when you apply this to a 
web search engine (but: very sparse vectors - most entries are zero).
Terms and documents as vectors
27

[PAGE 28]
UniversitÃ¤t Konstanz
ğ‘‰(d1) = (t1, t2, t3, t4)
ğ‘‰(d2) = (t1, t2, t3, t4)
â€¢
the vector value is derived from the term weights, e.g., the tf-idf weights
â€¢
the same term may receive different weights in different documents 
â†’Comparing documents and the query by comparing their vectors. 
Standard mathematics: vector cosine similarity
Documents as vectors
28

[PAGE 29]
UniversitÃ¤t Konstanz
Documents as vectors
29
tf-idf weights are used to characterize the documents. 
The denominator normalizes vectors of different lengths â†’allows us to 
compare documents of different lengths.

[PAGE 30]
UniversitÃ¤t Konstanz
Vector space proximity
30
Why normalizing for length?
Euclidean distance is large for vectors of different length.

[PAGE 31]
UniversitÃ¤t Konstanz
Length normalization
31
A vector is normalized by dividing each of its components by its length. 
(ğ‘‰(x)) = [ 3 1 2] 
|x| = 9 + 1 + 4 = 3,724
â†’Long and short documents now have comparable weights
ïƒ¥
=
i ix
x
2
2
ï²

[PAGE 32]
UniversitÃ¤t Konstanz
Length normalization
32
For length-normalized vectors, the cosine similarity is simply the dot 
product. 
Find more like this feature in 
search engines.

[PAGE 33]
UniversitÃ¤t Konstanz
Instead of comparing ğ‘‰(d1) to ğ‘‰(d2), we can compare ğ‘‰(d) to ğ‘‰(q), where 
ğ‘‰(d) is every document in our index, and ğ‘‰(q) is a query. 
Given a query q, all the documents in the collection can be ranked 
according to their similarity score sim(q,d). The higher the similarity, the 
more relevant the document to the query. 
Queries as vectors
33

[PAGE 34]
UniversitÃ¤t Konstanz
Example
34
Compute the cosine similarities between the three documents. 
For simplification: Only use tft,d with log-frequency weighting. 
term
d1
d2
d3
Arthur
115
58
20
Ford
10
7
11
Zaphod
2
0
6
Trillian
0
0
38

[PAGE 35]
UniversitÃ¤t Konstanz
Bereich fÃ¼r Partnerlogo
Auf eine Ausgewogenheit zwischen den gleichberechtigten 
Logos ist unbedingt zu achten. Gegebenenfalls muss die 
GrÃ¶ÃŸe des Partnerlogos angepasst werden.
Die GrÃ¶ÃŸe des Logos der UniversitÃ¤t Konstanz darf nicht 
verÃ¤ndert werden.
Annette Hautli-Janisz, Prof. Dr. 
cornlp-teaching@uni-passau.de
Comments?
Questions?
Thank you.