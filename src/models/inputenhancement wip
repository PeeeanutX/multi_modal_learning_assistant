
    def decide_action(self, query: str) -> str:
        """
        Decide whether to rewrite, decompose, disambiguate, or answer directly.
        Returns one of: 'rewrite', 'decompose', 'disambiguate', 'answer'.
        """
        try:
            prompt = PromptTemplate(
                input_variables=["query"],
                template="""
                You are an intelligent assistant that can refine user queries to improve retrieval and answer generation.
                Given the following user query, decide whether to rewrite, decompose, disambiguate, or answer directly.

                - rewrite: Rephrase the query for clarity.
                - decompose: Break down the query into simpler sub-queries.
                - disambiguate: Clarify any ambiguities in the query.
                - answer: Provide an answer directly without any refinement.

                Example 1:
                Query: "Tell me about Apple."
                Action: disambiguate

                Example 2:
                Query: "Explain the process of photosynthesis."
                Action: answer

                Query: "{query}"

                Please respond with exactly one of the following actions: rewrite, decompose, disambiguate, answer.
                Do not include any additional text or explanation.
                """
            )
            action_prompt = prompt.format(query=query)
            response = self.llm.invoke(action_prompt, stop=["\n"])

            if hasattr(response, 'content'):
                action = response.content.strip().lower()
            elif isinstance(response, str):
                action = response.strip().lower()
            else:
                logger.error(f"Unexpected response type: {type(response)}")
                action = 'answer'

            if action not in ['rewrite', 'decompose', 'disambiguate', 'answer']:
                logger.warning(f"Unexpected action '{action}' returned. Defaulting to 'answer'.")
                action = 'answer'
            logger.info(f"Decided action: {action}")
            return action
        except Exception as e:
            logger.error(f"Error during action decision: {e}")
            return 'answer'

    def refine_query(self, query: str, mode: str) -> str:
        """Refine the query based on the mode: 'rewrite', 'decompose', 'disambiguate'."""
        try:
            if mode not in ['rewrite', 'decompose', 'disambiguate']:
                logger.error(f"Invalid refinement mode: {mode}")
                return query

            prompt = PromptTemplate(
                input_variables=["query"],
                template=f"""
                You are an intelligent assistant specialized in {mode}ing user queries.
                Given the following user query, {mode} it to improve retrieval effectiveness.

                Original Query: "{query}"

                Refined Query:
                """
            )
            refined_query_prompt = prompt.format(query=query)
            refined_query_response = self.llm.invoke(refined_query_prompt, stop=["\n"])

            if hasattr(refined_query_response, 'content'):
                refined_query = refined_query_response.content.strip()
            elif isinstance(refined_query_response, str):
                refined_query = refined_query_response.strip()
            else:
                logger.error(f"Unexpected response type: {type(refined_query_response)}")
                refined_query = query

            logger.info(f"Refined query ({mode}): {refined_query}")
            return refined_query
        except Exception as e:
            logger.error(f"Error during query refinement ({mode}): {e}")
            return query

    def generate_response_from_docs(self, query: str, docs: List[str]) -> str:
        """Generate a response based on the refined query and retrieved documents."""
        try:
            context = "\n".join(docs)
            logger.debug(f"Combined context: {context}")

            chain_input = {
                "input": query,
                "context": context
            }

            result = self.chain.invoke(chain_input)
            answer = result.get("answer", "No answer found.")
            logger.info("Response generated successfully")
            return answer
        except Exception as e:
            logger.error(f"Error during response generation from docs: {e}")
            return "I'm sorry, I couldn't generate a response at this time."

    def run_trajectory(self, query: str, mode: str) -> Tuple[str, str]:
        """
        Run a single trajectory: refine the query based on mode, retrieve docs, and generate answer.
        Returns a tuple of (answer, mode).
        """
        refined_query = self.refine_query(query, mode)
        documents = self.retriever.invoke(refined_query)
        answer = self.generate_response_from_docs(refined_query, [doc.page_content for doc in documents])
        return answer, mode

    def select_best_answer(self, candidates: List[Tuple[str, str]], query: str) -> str:
        """
        Select the best answer from candidates based on semantic similarity to the query.
        """
        try:
            best_score = -float('inf')
            best_answer = "I'm sorry, I couldn't generate a suitable response."

            # Encode the query once for efficiency
            query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)

            # Extract all valid answers
            valid_answers = [answer for answer, _ in candidates if answer and answer.lower() != "no answer found."]
            modes = [mode for answer, mode in candidates if answer and answer.lower() != "no answer found."]

            if not valid_answers:
                logger.warning("No valid answers found among candidates.")
                return best_answer

            # Encode all answers in a single batch
            answer_embeddings = self.semantic_model.encode(valid_answers, convert_to_tensor=True)

            # Compute cosine similarities in a batch
            similarities = util.pytorch_cos_sim(query_embedding, answer_embeddings).squeeze(0)

            # Find the index of the best answer
            best_idx = similarities.argmax().item()
            best_score = similarities[best_idx].item()
            best_answer = valid_answers[best_idx]
            best_mode = modes[best_idx]

            logger.info(f"Selected answer from mode '{best_mode}' with similarity {best_score}: {best_answer}")

            return best_answer
        except Exception as e:
            logger.error(f"Error during answer selection: {e}")
            return "I'm sorry, I couldn't generate a suitable response."

    def generate_final_response(self, query: str) -> str:
        """Generate the final response using RQ-RAG approach"""
        try:
            logger.info(f"Generating final response for query: '{query}'")

            action = self.decide_action(query)

            candidates = []

            if action == 'answer':
                documents = self.retriever.invoke(query)
                answer = self.generate_response_from_docs(query, [doc.page_content for doc in documents])
                candidates.append((answer, 'answer'))
            elif action in ['rewrite', 'decompose', 'disambiguate']:
                answer, mode = self.run_trajectory(query, action)
                candidates.append((answer, mode))

                other_modes = ['rewrite', 'decompose', 'disambiguate']
                other_modes.remove(action)
                for mode in other_modes:
                    answer, _ = self.run_trajectory(query, mode)
                    candidates.append((answer, mode))
            else:
                documents = self.retriever.invoke(query)
                answer = self.generate_response_from_docs(query, [doc.page_content for doc in documents])
                candidates.append((answer, 'answer'))

            best_answer = self.select_best_answer(candidates, query)
            return best_answer

        except Exception as e:
            logger.error(f"Error during final response generation: {e}")
            return "I'm sorry, I couldn't generate a response at this time."

        self.semantic_model = SentenceTransformer('all-MiniLM-L6-v2')
        logger.info("SentenceTransformer model for semantic similarity initialized")